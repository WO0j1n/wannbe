# 트랜스포머 모델(GPT, BERT, T5 등)은 언어 모델 로 학습되었습니다 . 즉, 대량의 원시 텍스트를 사용하여 자기 지도 학습 방식으로 학습되었습니다 .
# 자기지도 학습은 모델의 입력값을 기반으로 목표값이 자동으로 계산되는 학습 방식입니다. 즉, 사람이 직접 데이터에 레이블을 지정할 필요가 없다는 뜻입니다!

# 사전 학습은 일반적으로 매우 방대한 양의 데이터를 사용하여 수행됩니다. 따라서 매우 큰 데이터셋이 필요하며, 학습에는 몇 주가 걸릴 수 있습니다.
# 반면, 미세 조정은 모델을 사전 학습한 후 수행하는 추가 학습입니다. 미세 조정을 하려면 먼저 사전 학습된 언어 모델을 확보한 다음, 특정 작업에 맞는 데이터셋으로 추가 학습을 진행합니다.

# 사전 학습된 모델은 미세 조정 데이터셋과 유사한 데이터셋으로 이미 학습되었습니다. 따라서 미세 조정 과정에서는 사전 학습 중에 초기 모델이 습득한 지식을 활용할 수 있습니다 (예를 들어, 자연어 처리 문제의 경우 사전 학습된 모델은 작업에 사용되는 언어에 대한 통계적 이해를 갖고 있을 것입니다).
# 사전 학습된 모델은 이미 많은 데이터로 학습되었기 때문에, 미세 조정을 위해서는 훨씬 적은 양의 데이터로도 괜찮은 결과를 얻을 수 있습니다.


# 요약하자면, 사전 학습은 대량의 원시 텍스트 데이터를 사용하여 모델을 일반적인 언어 이해 능력을 갖추도록 하는 과정이며,
# 미세 조정은 특정 작업에 맞게 사전 학습된 모델을 추가로 학습시키는 과정입니다.
# 이러한 접근 방식은 모델이 다양한 작업에 적응할 수 있도록 하여, 자연어 처리 분야에서 매우 효과적인 방법으로 자리잡고 있습니다

# 주로 인코더와 디코더로 구성되어 있다.
    # 인코더: 인코더는 입력을 받아 그 입력의 표현(특징)을 구축합니다. 즉, 이 모델은 입력으로부터 의미를 파악하도록 최적화되어 있습니다.
    # 디코더: 디코더는 인코더의 표현(특징)과 다른 입력들을 이용하여 목표 시퀀스를 생성합니다. 즉, 이 모델은 출력을 생성하는 데 최적화되어 있습니다.

# 이러한 인코더와 디코더 그리고 이 둘을 다 활용하는 모델들은 서로 다른 각각의 작업에 적합합니다.
    # 인코더만 사용하는 모델: BERT, RoBERTa, DistilBERT - 주로 텍스트 분류, 감정 분석, 질의응답 등과 같은 작업에 사용됩니다.
    # 디코더만 사용하는 모델: GPT 시리즈 - 주로 텍스트 생성, 대화형 AI, 자동 완성 등과 같은 작업에 사용됩니다.
    # 인코더-디코더 모델: T5, BART - 주로 번역, 요약, 텍스트 생성 등과 같은 작업에 사용됩니다.


# 아키텍처 vs 체크포인트 vs ahepf
# 아키텍처: 모델의 구조와 설계를 의미합니다. 예를 들어, BERT, GPT-3, T5 등은 각각 고유한 아키텍처를 가지고 있습니다.
# 체크포인트: 특정 시점에서 모델의 가중치와 매개변수를 저장한 파일입니다. 체크포인트는 모델을 재사용하거나 미세 조정할 때 사용됩니다. -> 주넝자